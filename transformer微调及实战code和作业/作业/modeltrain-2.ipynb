{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34647ce0-2267-422a-949e-577b0be8f9ba",
   "metadata": {},
   "source": [
    "## 如何开始训练？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122e75ad-40ad-4ad9-953b-de47b2d769cd",
   "metadata": {},
   "source": [
    "### pytorch 补充"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a06b54-6c9d-4446-9bf1-ccbf381c80fc",
   "metadata": {},
   "source": [
    "transformer的库相对来说封装完整，我们不必过度依赖pytorch进行训练过程，但数据的加载、模型参数调优等等还需要通过pytorch实现。使用 Pytorch 进行计算的好处是更高效的执行速度，尤其当张量存储的数据很多时，而且还可以借助 GPU 进一步提高计算速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39130d6-1a99-4145-b0bb-c5702da5f591",
   "metadata": {},
   "source": [
    "张量 (Tensor) 是深度学习的基础，例如常见的 0 维张量称为标量 (scalar)、1 维张量称为向量 (vector)、2 维张量称为矩阵 (matrix)。Pytorch 本质上就是一个基于张量的数学计算工具包，它提供了多种方式来创建张量："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d11d62-a218-4fc7-b8f3-5191531d6a43",
   "metadata": {},
   "source": [
    "针对张量的计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60715092-29e7-423e-b097-5a6b024f7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------a----------\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "----------b----------\n",
      "tensor([[0.2977, 0.0095, 0.3499],\n",
      "        [0.2987, 0.2878, 0.1401]])\n",
      "----------c----------\n",
      "tensor([[ 0.5798, -0.0051, -1.5515],\n",
      "        [-0.3403, -0.5510, -0.7097]])\n",
      "----------d----------\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "----------e----------\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n",
      "----------f----------\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a=torch.empty(2, 3) # empty tensor (uninitialized), shape (2,3)\n",
    "print('----------a----------')\n",
    "print(a)\n",
    "b=torch.rand(2, 3) # random tensor, each value taken from [0,1)\n",
    "print('----------b----------')\n",
    "print(b)\n",
    "print('----------c----------')\n",
    "c= torch.randn(2, 3) # random tensor, each value taken from standard normal distribution\n",
    "print(c)\n",
    "print('----------d----------')\n",
    "d= torch.zeros(2, 3, dtype=torch.long) # long integer zero tensor\n",
    "print(d)\n",
    "print('----------e----------')\n",
    "e= torch.zeros(2, 3, dtype=torch.double) # double float zero tensor\n",
    "print(e)\n",
    "print('----------f----------')\n",
    "f= torch.arange(10)\n",
    "print(f)\n",
    "print('-------array1---------')\n",
    "## 也可以直接基于numpy转化\n",
    "array = [[1.0, 3.8, 2.1], [8.6, 4.0, 2.4]]\n",
    "print(torch.tensor(array))\n",
    "print('-------array2---------')\n",
    "array = np.array([[1.0, 3.8, 2.1], [8.6, 4.0, 2.4]])\n",
    "print(torch.from_numpy(array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c17c7df-addf-4950-ac68-2f8d312bd81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], dtype=torch.float64)\n",
      "tensor([-3., -3., -3.], dtype=torch.float64)\n",
      "tensor([ 4., 10., 18.], dtype=torch.float64)\n",
      "tensor([0.2500, 0.4000, 0.5000], dtype=torch.float64)\n",
      "tensor(32., dtype=torch.float64)\n",
      "tensor([1., 2., 3., 4., 5., 6.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.double)\n",
    "y = torch.tensor([4, 5, 6], dtype=torch.double)\n",
    "# 加减乘除\n",
    "print(x + y)\n",
    "print(x - y)\n",
    "print(x * y)\n",
    "print(x / y)\n",
    "print(x.dot(y))\n",
    "a=torch.cat((x, y),dim=0)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04411b90-95c3-4e99-89f2-2dd0ef98aa48",
   "metadata": {},
   "source": [
    "### pytorch支持梯度的自动计算。\n",
    "可以自动计算一个函数关于一个变量在某一取值下的导数，从而基于梯度对参数进行优化，这就是机器学习中的训练过程。使用 Pytorch 计算梯度非常容易，只需要执行 tensor.backward()，就会自动通过反向传播 (Back Propogation) 算法完成，后面我们在训练模型时就会用到该函数。同时，还可以通过view，reshape，permute，transpose等函数实现张量尺寸的变化重构。以及实现索引、切片、等功能。 请参考https://pytorch.org/docs/stable/index.html，以及课下资料进行补充学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "406127e3-fa0a-44cb-9241-fb206edd07d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.], grad_fn=<MulBackward0>)\n",
      "tensor([1.]) tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = torch.tensor([3.], requires_grad=True)\n",
    "z = (x + y) * (y - 2)\n",
    "print(z)\n",
    "z.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cadff98-63b5-42cd-ba38-3f217377701c",
   "metadata": {},
   "source": [
    "### 数据处理（dataloaders）\n",
    "在实际训练模型时，我们都需要先将数据集切分为很多的 mini-batches，然后按批 (batch) 将样本送入模型，并且循环这一过程，每一个完整遍历所有样本的循环称为一个 epoch。\n",
    "\n",
    "Pytorch 提供了 DataLoader 类专门负责处理这些操作，除了基本的 dataset（数据集）和 batch_size （batch 大小）参数以外，还有以下常用参数：\n",
    "\n",
    "shuffle：是否打乱数据集；\n",
    "sampler：采样器，也就是一个索引上的迭代器；\n",
    "collate_fn：批处理函数，用于对采样出的一个 batch 中的样本进行处理（比如当最后一个batch数量不足，进行填0）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aed040-ef55-473b-854d-804b228928bc",
   "metadata": {},
   "source": [
    "当需要对数据进行采样时，还需要定义sampler类，作为采样器。比如我们可以乱序地为每一轮数据训练读入数据。\n",
    "\n",
    "### 数据加载实例：\n",
    "\n",
    "- 导入了必要的库和模块。\n",
    "\n",
    "- 加载了FashionMNIST数据集的训练集和测试集，并将图片转换为PyTorch张量。\n",
    "\n",
    "- 创建了训练集和测试集的DataLoader，用于在训练和测试时批量获取数据。\n",
    "\n",
    "- 从DataLoader中获取了第一批数据的特征和标签，并打印了它们的形状。\n",
    "\n",
    "- 取出了特征批次中的第一张图片和对应的标签，并打印了图片的形状和标签的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951814d-45c3-4f21-b729-d40b2f02ffac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca9ee03a-f429-447c-8c04-405d7986b596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 26421880/26421880 [00:33<00:00, 787865.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 29515/29515 [00:00<00:00, 185793.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 4422102/4422102 [01:22<00:00, 53778.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 5148/5148 [00:00<00:00, 3987493.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n",
      "torch.Size([28, 28])\n",
      "Label: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# 从torchvision库中导入FashionMNIST数据集\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",  # 指定数据存储的目录\n",
    "    train=True,  # 指定加载训练集\n",
    "    download=True,  # 如果数据集不存在，则下载数据集\n",
    "    transform=ToTensor()  # 将图片转换为PyTorch张量\n",
    ")\n",
    "\n",
    "# 从torchvision库中导入FashionMNIST数据集，用于测试集\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",  # 指定数据存储的目录\n",
    "    train=False,  # 指定加载测试集\n",
    "    download=True,  # 如果数据集不存在，则下载数据集\n",
    "    transform=ToTensor()  # 将图片转换为PyTorch张量\n",
    ")\n",
    "\n",
    "# 创建训练数据的DataLoader，设置批处理大小为64，并且数据在每个epoch开始时随机打乱\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "# 创建测试数据的DataLoader，设置批处理大小为64，并且数据在每个epoch开始时随机打乱\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# 使用DataLoader的迭代器获取第一批数据的特征和标签\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "# 打印特征批次的形状，即每批有多少张图片，每张图片的尺寸是多少\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# 打印标签批次的形状，即每批有多少个标签\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "# 从特征批次中取出第一张图片，并使用squeeze()方法去除维度为1的轴，使其成为二维数组\n",
    "img = train_features[0].squeeze()\n",
    "# 从标签批次中取出第一个标签\n",
    "label = train_labels[0]\n",
    "# 打印图片的形状，即图片的尺寸\n",
    "print(img.shape)\n",
    "# 打印标签的值\n",
    "print(f\"Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a832fa-9939-4731-8ea3-52d8b67bf46e",
   "metadata": {},
   "source": [
    "### 下面我们使用PyTorch框架实现了简单的神经网络训练和测试流程，主要用于图像分类任务。代码的主要结构和功能如下：\n",
    "\n",
    "### 环境设置：\n",
    "\n",
    "检查是否有可用的CUDA设备（GPU），如果有则使用GPU进行计算，否则使用CPU。\n",
    "\n",
    "### 数据加载：\n",
    "\n",
    "使用datasets.FashionMNIST加载FashionMNIST数据集，这是一个包含10个类别的服装图像数据集。 数据集被分为训练集和测试集，并且通过ToTensor()转换为PyTorch张量。 使用DataLoader创建数据加载器，以便在训练和测试时批量获取数据。\n",
    "\n",
    "### 模型定义：\n",
    "\n",
    "定义一个名为NeuralNetwork的类，它继承自nn.Module，表示一个简单的前馈神经网络。 网络包含一个展平层（nn.Flatten），将28x28的图像展平为784维的向量。 接着是三个全连接层（nn.Linear），中间两个全连接层后跟ReLU激活函数，最后一个全连接层后跟Dropout层以减少过拟合。\n",
    "\n",
    "### 训练和测试函数：\n",
    "\n",
    "train_loop：定义训练循环，负责模型的训练过程，包括前向传播、计算损失、反向传播和参数更新。 test_loop：定义测试循环，负责模型的评估过程，计算测试集上的损失和准确率。\n",
    "\n",
    "### 优化器和损失函数：\n",
    "\n",
    "使用nn.CrossEntropyLoss作为损失函数，适用于多分类问题。 使用torch.optim.AdamW作为优化器，这是一种带有权重衰减的Adam优化算法。\n",
    "\n",
    "### 训练过程：\n",
    "\n",
    "通过循环epochs次，每次循环中调用train_loop进行训练，调用test_loop进行测试，并打印出每个周期的损失和准确率。 输出：\n",
    "\n",
    "在训练结束后，打印\"Done!\"表示训练完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbef488-1148-4cb3-968e-103b5592fa5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9abb08c8-1cd2-46d2-98e8-b36e4162b7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.983582  [ 6400/60000]\n",
      "loss: 0.872947  [12800/60000]\n",
      "loss: 0.538679  [19200/60000]\n",
      "loss: 0.889003  [25600/60000]\n",
      "loss: 0.480309  [32000/60000]\n",
      "loss: 0.538123  [38400/60000]\n",
      "loss: 0.856580  [44800/60000]\n",
      "loss: 0.611855  [51200/60000]\n",
      "loss: 0.475762  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.464763 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.606370  [ 6400/60000]\n",
      "loss: 0.521568  [12800/60000]\n",
      "loss: 0.557305  [19200/60000]\n",
      "loss: 0.790215  [25600/60000]\n",
      "loss: 0.476856  [32000/60000]\n",
      "loss: 0.451210  [38400/60000]\n",
      "loss: 0.938261  [44800/60000]\n",
      "loss: 0.653995  [51200/60000]\n",
      "loss: 0.495939  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.418934 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.579286  [ 6400/60000]\n",
      "loss: 0.642542  [12800/60000]\n",
      "loss: 0.432076  [19200/60000]\n",
      "loss: 0.712900  [25600/60000]\n",
      "loss: 0.340836  [32000/60000]\n",
      "loss: 0.378215  [38400/60000]\n",
      "loss: 0.820740  [44800/60000]\n",
      "loss: 0.497841  [51200/60000]\n",
      "loss: 0.483081  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.365288 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# 检查是否有可用的CUDA设备，如果有则使用GPU，否则使用CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "# 加载FashionMNIST训练数据集，并将图片转换为Tensor\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# 加载FashionMNIST测试数据集，并将图片转换为Tensor\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# 设置学习率、批处理大小和训练周期\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "\n",
    "# 创建训练数据的DataLoader\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "# 创建测试数据的DataLoader\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# 定义神经网络结构\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # 将图片展平为一维向量\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 定义一个包含两个隐藏层的前馈神经网络\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.Dropout(p=0.2)  # 在最后一个全连接层后添加Dropout防止过拟合\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播过程\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# 实例化模型并将其移动到GPU或CPU\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# 定义训练循环函数\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    for batch, (X, y) in enumerate(dataloader, start=1):\n",
    "        X, y = X.to(device), y.to(device)  # 将数据移动到GPU或CPU\n",
    "        # 计算预测结果和损失\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# 定义测试循环函数\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 在评估模式下不计算梯度\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(dim=-1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练和测试模型\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa733bed-2cb8-483c-9607-352af7b7eb29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
