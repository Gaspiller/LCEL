{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47073646-e5ea-4ec9-82b1-6af94be39522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 512])\n",
      "torch.Size([2, 6, 512])\n",
      "TransformerDecoder(\n",
      "  (input_emb): Embedding(1, 8)\n",
      "  (position_emb): Embedding(20, 8)\n",
      "  (layers): TransformerDecoderLayers(\n",
      "    (wk): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (wv): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): ResidualsNorm(\n",
      "        (block): MultiHeadSelfAttention(\n",
      "          (wq): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wk): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wv): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wo): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): ResidualsNorm(\n",
      "        (block): MultiHeadEncoderDecoderAttention(\n",
      "          (wo): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): ResidualsNorm(\n",
      "        (block): FFN(\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (3): ResidualsNorm(\n",
      "        (block): MultiHeadSelfAttention(\n",
      "          (wq): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wk): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wv): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wo): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (4): ResidualsNorm(\n",
      "        (block): MultiHeadEncoderDecoderAttention(\n",
      "          (wo): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (5): ResidualsNorm(\n",
      "        (block): FFN(\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (6): ResidualsNorm(\n",
      "        (block): MultiHeadSelfAttention(\n",
      "          (wq): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wk): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wv): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wo): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (7): ResidualsNorm(\n",
      "        (block): MultiHeadEncoderDecoderAttention(\n",
      "          (wo): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (8): ResidualsNorm(\n",
      "        (block): FFN(\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (9): ResidualsNorm(\n",
      "        (block): MultiHeadSelfAttention(\n",
      "          (wq): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wk): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wv): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wo): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (10): ResidualsNorm(\n",
      "        (block): MultiHeadEncoderDecoderAttention(\n",
      "          (wo): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (11): ResidualsNorm(\n",
      "        (block): FFN(\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (12): ResidualsNorm(\n",
      "        (block): MultiHeadSelfAttention(\n",
      "          (wq): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wk): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wv): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (wo): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (13): ResidualsNorm(\n",
      "        (block): MultiHeadEncoderDecoderAttention(\n",
      "          (wo): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (14): ResidualsNorm(\n",
      "        (block): FFN(\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=32, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=32, out_features=8, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# N: 表示批次大小（batch size），即一次处理的样本数量。\n",
    "# T: 表示序列长度（sequence length），可以是输入序列的长度，也可以是输出序列的长度。在编码器中，它通常表示输入序列的长度；在解码器中，它表示目标序列的长度。\n",
    "# E: 表示嵌入维度（embedding size）或特征维度（feature size），即每个向量的大小。\n",
    "\n",
    "def qkv_attention_value(q, k, v, mask=False):\n",
    "    \"\"\"\n",
    "    计算attention value\n",
    "    :param q: [N,T1,E] or [N,h,T1,E]\n",
    "    :param k: [N,T2,E] or [N,h,T2,E]\n",
    "    :param v: [N,T2,E] or [N,h,T2,E]\n",
    "    :param mask: True or False or Tensor\n",
    "    :return: [N,T1,E] or [N,h,T1,E]\n",
    "    \"\"\"\n",
    "    # 2. 计算q和k之间的相关性->F函数\n",
    "    k = torch.transpose(k, dim0=-2, dim1=-1)  # [??, T2, E] --> [??, E, T2]\n",
    "    # matmul([??,T1,E], [??,E,T2])\n",
    "    scores = torch.matmul(q, k)  # [??,T1,T2]\n",
    "\n",
    "    if isinstance(mask, bool):\n",
    "        if mask:\n",
    "            _shape = scores.shape\n",
    "            mask = torch.ones((_shape[-2], _shape[-1]))\n",
    "            mask = torch.triu(mask, diagonal=1) * -10000\n",
    "            mask = mask[None][None]\n",
    "        else:\n",
    "            mask = None\n",
    "    if mask is not None:\n",
    "        scores = scores + mask\n",
    "\n",
    "    # 3. 转换为权重\n",
    "    alpha = torch.softmax(scores, dim=-1)  # [??,T1,T2]\n",
    "\n",
    "    # 4. 值的合并\n",
    "    # matmul([??,T1,T2], [??,T2,E])\n",
    "    v = torch.matmul(alpha, v)  # [??,T1,E]\n",
    "    return v\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_header):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert hidden_size % num_header == 0, f\"header的数目没办法整除:{hidden_size}, {num_header}\"\n",
    "\n",
    "        self.hidden_size = hidden_size  # 就是向量维度大小，也就是E\n",
    "        self.num_header = num_header  # 头的数目\n",
    "\n",
    "        self.wq = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
    "        )\n",
    "        self.wk = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
    "        )\n",
    "        self.wv = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
    "        )\n",
    "        self.wo = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "#    在这里通过reshape 将传入的q k v进行分割\n",
    "    def split(self, vs):\n",
    "        n, t, e = vs.shape\n",
    "        vs = torch.reshape(vs, shape=(n, t, self.num_header, e // self.num_header))\n",
    "        vs = torch.permute(vs, dims=(0, 2, 1, 3))\n",
    "        return vs\n",
    "\n",
    "    def forward(self, x, attention_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        前向过程\n",
    "        :param x: [N,T,E] 输入向量\n",
    "        :param attention_mask: [N,T,T] mask矩阵\n",
    "        :return: [N,T,E] 输出向量\n",
    "        \"\"\"\n",
    "        # 1. 获取q、k、v\n",
    "        q = self.wq(x)  # [n,t,e]\n",
    "        k = self.wk(x)  # [n,t,e]\n",
    "        v = self.wv(x)  # [n,t,e]\n",
    "        q = self.split(q)  # [n,t,e] --> [n,h,t,v]  e=h*v h就是head的数目，v就是每个头中self-attention的维度大小\n",
    "        k = self.split(k)  # [n,t,e] --> [n,h,t,v]  e=h*v\n",
    "        v = self.split(v)  # [n,t,e] --> [n,h,t,v]  e=h*v\n",
    "\n",
    "        # 计算attention value\n",
    "        v = qkv_attention_value(q, k, v, attention_mask)\n",
    "\n",
    "        # 5. 输出\n",
    "        v = torch.permute(v, dims=(0, 2, 1, 3))  # [n,h,t,v] --> [n,t,h,v]\n",
    "        n, t, _, _ = v.shape\n",
    "        v = torch.reshape(v, shape=(n, t, -1))  # [n,t,h,v] -> [n,t,e]\n",
    "        v = self.wo(v)  # 多个头之间的特征组合合并\n",
    "        return v\n",
    "\n",
    "\n",
    "class MultiHeadEncoderDecoderAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_header):\n",
    "        super(MultiHeadEncoderDecoderAttention, self).__init__()\n",
    "        assert hidden_size % num_header == 0, f\"header的数目没办法整除:{hidden_size}, {num_header}\"\n",
    "\n",
    "        self.hidden_size = hidden_size  # 就是向量维度大小，也就是E\n",
    "        self.num_header = num_header  # 头的数目\n",
    "\n",
    "        self.wo = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def split(self, vs):\n",
    "        n, t, e = vs.shape\n",
    "        vs = torch.reshape(vs, shape=(n, t, self.num_header, e // self.num_header))\n",
    "        vs = torch.permute(vs, dims=(0, 2, 1, 3))\n",
    "        return vs\n",
    "\n",
    "    def forward(self, q, encoder_k, encoder_v, encoder_attention_mask, **kwargs):\n",
    "        \"\"\"\n",
    "        编码器解码器attention\n",
    "        :param q: [N,T1,E]\n",
    "        :param encoder_k: [N,T2,E]\n",
    "        :param encoder_v: [N,T2,E]\n",
    "        :param encoder_attention_mask: [N,1,T2,T2]\n",
    "        :return: [N,T1,E]\n",
    "        \"\"\"\n",
    "        q = self.split(q)  # [n,t,e] --> [n,h,t,v]  e=h*v h就是head的数目，v就是每个头中self-attention的维度大小\n",
    "        k = self.split(encoder_k)  # [n,t,e] --> [n,h,t,v]  e=h*v\n",
    "        v = self.split(encoder_v)  # [n,t,e] --> [n,h,t,v]  e=h*v\n",
    "\n",
    "        # 计算attention value\n",
    "        v = qkv_attention_value(q, k, v, mask=encoder_attention_mask)\n",
    "\n",
    "        # 5. 输出\n",
    "        v = torch.permute(v, dims=(0, 2, 1, 3))  # [n,h,t,v] --> [n,t,h,v]\n",
    "        n, t, _, _ = v.shape\n",
    "        v = torch.reshape(v, shape=(n, t, -1))  # [n,t,h,v] -> [n,t,e]\n",
    "        v = self.wo(v)  # 多个头之间的特征组合合并\n",
    "        return v\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(FFN, self).__init__()\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.ffn(x)\n",
    "\n",
    "\n",
    "class ResidualsNorm(nn.Module):\n",
    "    def __init__(self, block, hidden_size):\n",
    "        super(ResidualsNorm, self).__init__()\n",
    "        self.block = block\n",
    "        self.norm = nn.LayerNorm(normalized_shape=hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        z = self.block(x, **kwargs)\n",
    "        z = self.relu(x + z)\n",
    "        z = self.norm(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class TransformerEncoderLayers(nn.Module):\n",
    "    def __init__(self, hidden_size, num_header, encoder_layers):\n",
    "        super(TransformerEncoderLayers, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(encoder_layers):\n",
    "            layer = [\n",
    "                ResidualsNorm(\n",
    "                    block=MultiHeadSelfAttention(hidden_size=hidden_size, num_header=num_header),\n",
    "                    hidden_size=hidden_size\n",
    "                ),\n",
    "                ResidualsNorm(\n",
    "                    block=FFN(hidden_size=hidden_size),\n",
    "                    hidden_size=hidden_size\n",
    "                )\n",
    "            ]\n",
    "            layers.extend(layer)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        attention_mask = torch.unsqueeze(attention_mask, dim=1)  # 增加header维度\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_header, max_seq_length, encoder_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.input_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)\n",
    "        self.position_emb = nn.Embedding(num_embeddings=max_seq_length, embedding_dim=hidden_size)\n",
    "        self.layers = TransformerEncoderLayers(hidden_size, num_header, encoder_layers)\n",
    "\n",
    "    def forward(self, input_token_ids, input_position_ids, input_mask):\n",
    "        \"\"\"\n",
    "        前向过程\n",
    "        :param input_token_ids: [N,T] long类型的token id\n",
    "        :param input_position_ids: [N,T] long类型的位置id\n",
    "        :param input_mask: [N,T,T] float类型的mask矩阵\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 1. 获取token的embedding\n",
    "        inp_embedding = self.input_emb(input_token_ids)  # [N,T,E]\n",
    "\n",
    "        # 2. 获取位置embedding\n",
    "        position_embedding = self.position_emb(input_position_ids)\n",
    "\n",
    "        # 3. 合并embedding\n",
    "        emd = inp_embedding + position_embedding\n",
    "\n",
    "        # 4. 输入到attention提取特征\n",
    "        feat_emd = self.layers(emd, attention_mask=input_mask)\n",
    "\n",
    "        return feat_emd\n",
    "\n",
    "\n",
    "class TransformerDecoderLayers(nn.Module):\n",
    "    def __init__(self, hidden_size, num_header, decoder_layers):\n",
    "        super(TransformerDecoderLayers, self).__init__()\n",
    "\n",
    "        self.wk = nn.Linear(hidden_size, hidden_size)\n",
    "        self.wv = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(decoder_layers):\n",
    "            layer = [\n",
    "                ResidualsNorm(\n",
    "                    block=MultiHeadSelfAttention(hidden_size=hidden_size, num_header=num_header),\n",
    "                    hidden_size=hidden_size\n",
    "                ),\n",
    "                ResidualsNorm(\n",
    "                    block=MultiHeadEncoderDecoderAttention(hidden_size=hidden_size, num_header=num_header),\n",
    "                    hidden_size=hidden_size\n",
    "                ),\n",
    "                ResidualsNorm(\n",
    "                    block=FFN(hidden_size=hidden_size),\n",
    "                    hidden_size=hidden_size\n",
    "                )\n",
    "            ]\n",
    "            layers.extend(layer)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, encoder_outputs=None, encoder_attention_mask=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        :param x: [N,T2,E]\n",
    "        :param encoder_outputs: [N,T1,E]\n",
    "        :param encoder_attention_mask: [N,1,T1]\n",
    "        :param attention_mask: [N,T2,T2]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        attention_mask = torch.unsqueeze(attention_mask, dim=1)  # 增加header维度 [N,T2,T2] -> [N,1,T2,T2]\n",
    "        encoder_attention_mask = torch.unsqueeze(encoder_attention_mask, dim=1)  # 增加header维度 [N,1,T1] -> [N,1,1,T1]\n",
    "        k = self.wk(encoder_outputs)  # [N,T1,E]\n",
    "        v = self.wv(encoder_outputs)  # [N,T1,E]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                encoder_k=k, encoder_v=v, encoder_attention_mask=encoder_attention_mask,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_header, max_seq_length, decoder_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.input_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)\n",
    "        self.position_emb = nn.Embedding(num_embeddings=max_seq_length, embedding_dim=hidden_size)\n",
    "        self.layers = TransformerDecoderLayers(hidden_size, num_header, decoder_layers)\n",
    "\n",
    "    def forward(self, input_token_ids, input_position_ids, input_mask, encoder_outputs, encoder_attention_mask):\n",
    "        \"\"\"\n",
    "        前向过程\n",
    "        :param input_token_ids: [N,T] long类型的token id\n",
    "        :param input_position_ids: [N,T] long类型的位置id\n",
    "        :param input_mask: [N,T,T] float类型的mask矩阵\n",
    "        :param encoder_outputs: [N,T1,E] 编码器的输出状态信息\n",
    "        :param encoder_attention_mask: [N,T1,T1] 编码器的输入mask信息\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # 1. 获取token的embedding\n",
    "            inp_embedding = self.input_emb(input_token_ids)  # [N,T,E]\n",
    "\n",
    "            # 2. 获取位置embedding\n",
    "            position_embedding = self.position_emb(input_position_ids)\n",
    "\n",
    "            # 3. 合并embedding\n",
    "            emd = inp_embedding + position_embedding\n",
    "\n",
    "            # 4. 输入到attention提取特征\n",
    "            feat_emd = self.layers(\n",
    "                emd, encoder_outputs=encoder_outputs,\n",
    "                encoder_attention_mask=encoder_attention_mask, attention_mask=input_mask\n",
    "            )\n",
    "\n",
    "            return feat_emd\n",
    "        else:\n",
    "            raise ValueError(\"当前模拟代码不实现推理过程，仅实现training过程\")\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def forward(self, encoder_input_ids, encoder_lengths, label_ids=None, label_lengths=None):\n",
    "        \"\"\"\n",
    "        :param encoder_input_ids: 编码器输入token id: [N,T1]\n",
    "        :param encoder_lengths: 编码器输入的文本实际长度值, [N,] 也可以直接外部传入mask等信息，但是至少要有代码实现从length到mask的转换\n",
    "        :param label_ids: 模型预测期望输出的token id: [N,T2]\n",
    "        :param label_lengths: 模型预测期望输出文本实际长度, [N,]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "def transformer():\n",
    "    encoder = TransformerEncoder(vocab_size=1000, hidden_size=512, num_header=8, max_seq_length=1024, encoder_layers=6)\n",
    "    decoder = TransformerDecoder(vocab_size=1000, hidden_size=512, num_header=8, max_seq_length=1024, decoder_layers=6)\n",
    "\n",
    "    input_token_ids = torch.tensor([\n",
    "        [100, 102, 108, 253, 125],  # 第一个样本实际长度为5\n",
    "        [254, 125, 106, 0, 0]  # 第二个样本实际长度为3\n",
    "    ])\n",
    "    input_position_ids = torch.tensor([\n",
    "        [0, 1, 2, 3, 4],\n",
    "        [0, 1, 2, 3, 4]\n",
    "    ])\n",
    "    input_mask = torch.tensor([\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        ],\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "            [-10000.0, -10000.0, -10000.0, 0.0, -10000.0],\n",
    "            [-10000.0, -10000.0, -10000.0, -10000.0, 0.0],\n",
    "        ],\n",
    "    ])\n",
    "    encoder_attention_mask = torch.tensor([\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0]  # 表示第一个样本的解码器中第一个时刻和编码器的各个时刻之间的mask值\n",
    "        ],\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0]  # 是因为编码器的输入中，最后两个位置是填充\n",
    "        ],\n",
    "    ])\n",
    "\n",
    "    input_decoder_token_ids = torch.tensor([\n",
    "        [251, 235, 124, 321, 25, 68],\n",
    "        [351, 235, 126, 253, 0, 0]\n",
    "    ])\n",
    "    input_decoder_position_ids = torch.tensor([\n",
    "        [0, 1, 2, 3, 4, 5],\n",
    "        [0, 1, 2, 3, 4, 5]\n",
    "    ])\n",
    "    input_decoder_mask = torch.tensor([\n",
    "        [\n",
    "            [0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        ],\n",
    "        [\n",
    "            [0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "            [-10000.0, -10000.0, -10000.0, -10000.0, 0.0, -10000.0],\n",
    "            [-10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0]\n",
    "        ],\n",
    "    ])\n",
    "\n",
    "    encoder_outputs = encoder(input_token_ids, input_position_ids, input_mask)\n",
    "    print(encoder_outputs.shape)\n",
    "\n",
    "    decoder_outputs = decoder(\n",
    "        input_token_ids=input_decoder_token_ids,\n",
    "        input_position_ids=input_decoder_position_ids,\n",
    "        input_mask=input_decoder_mask,\n",
    "        encoder_outputs=encoder_outputs,\n",
    "        encoder_attention_mask=encoder_attention_mask\n",
    "    )\n",
    "    print(decoder_outputs.shape)\n",
    "\n",
    "def model_structure(model):\n",
    "    blank = ' '\n",
    "    print('-' * 90)\n",
    "    print('|' + ' ' * 11 + 'weight name' + ' ' * 10 + '|' \\\n",
    "          + ' ' * 15 + 'weight shape' + ' ' * 15 + '|' \\\n",
    "          + ' ' * 3 + 'number' + ' ' * 3 + '|')\n",
    "    print('-' * 90)\n",
    "    num_para = 0\n",
    "    type_size = 1  # 如果是浮点数就是4\n",
    " \n",
    "    for index, (key, w_variable) in enumerate(model.named_parameters()):\n",
    "        if len(key) <= 30:\n",
    "            key = key + (30 - len(key)) * blank\n",
    "        shape = str(w_variable.shape)\n",
    "        if len(shape) <= 40:\n",
    "            shape = shape + (40 - len(shape)) * blank\n",
    "        each_para = 1\n",
    "        for k in w_variable.shape:\n",
    "            each_para *= k\n",
    "        num_para += each_para\n",
    "        str_num = str(each_para)\n",
    "        if len(str_num) <= 10:\n",
    "            str_num = str_num + (10 - len(str_num)) * blank\n",
    " \n",
    "        print('| {} | {} | {} |'.format(key, shape, str_num))\n",
    "    print('-' * 90)\n",
    "    print('The total number of parameters: ' + str(num_para))\n",
    "    print('The parameters of Model {}: {:4f}M'.format(model._get_name(), num_para * type_size / 1000 / 1000))\n",
    "    print('-' * 90)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    transformer()\n",
    "    net=TransformerDecoder(1,8,8,20,5)\n",
    "    print(net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b7017-b05f-4bdb-b79e-bb44e8449c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
