{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "env_config = dotenv_values(\".env\")\n",
    "API_KEY = env_config[\"API_KEY\"]\n",
    "BASE_URL = env_config[\"BASE_URL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain 六个模块\n",
    "- Model \n",
    "    - LLM model\n",
    "    - chat model\n",
    "    - emb model\n",
    "- prompt\n",
    "    - input\n",
    "    - output parser prompt\n",
    "- chain\n",
    "    - 自己 LECL 定义\n",
    "        -  prompt | chat | output_part\n",
    "    -  import LLMChain\n",
    "    - xxxxx\n",
    "- Retrieval\n",
    "    - database\n",
    "    - retriver\n",
    "- memory\n",
    "- agent\n",
    "\n",
    "帮我们写了一些胶水代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 普通 LLM model \n",
    "from langchain.llms import OpenAI, HuggingFaceHub, HuggingFaceTextGenInference\n",
    "\n",
    "llm = OpenAI(api_key=API_KEY, base_url=BASE_URL, model_name=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "llm(\"大模型是什么？\")\n",
    "\n",
    "\n",
    "# # 如果是其他的模型，这里可以自己去设置一些 hugging\n",
    "# llm = HuggingFaceHub(repo_id =\"NYTK/text-generation-news-gpt2-small-hungarian\",\n",
    "#                       huggingfacehub_api_token=\"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\", \n",
    "#                       model_kwargs={\"temperature\": 0.7}) \n",
    "# llm(\"大模型是什么?\")\n",
    "\n",
    "\n",
    "# # # 使用 本地模型\n",
    "# llm = HuggingFaceTextGenInference(\n",
    "#                 inference_server_url=\"http://localhost:8010/\",\n",
    "#                 max_new_tokens=512,\n",
    "#                 top_k=10,\n",
    "#                 top_p=0.95,\n",
    "#                 typical_p=0.95,\n",
    "#                 temperature=0.01,\n",
    "#                 repetition_penalty=1.03,\n",
    "#             )\n",
    "# print(llm(\"What is Deep Learning?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 chat 模型\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "chat = ChatOpenAI(api_key=API_KEY, \n",
    "                  base_url=BASE_URL, \n",
    "                  model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "ans = chat.invoke(\"帮我写一个计算机相关的笑话，不要超过 100 个字\")\n",
    "print(ans.content)\n",
    "\n",
    "# # 使用 embedding 模型\n",
    "emb_model = OpenAIEmbeddings(api_key=env_config.get(\"API_KEY\"), \n",
    "                             base_url=env_config.get(\"BASE_URL\"))\n",
    "\n",
    "# emb_content = emb_model.embed_query(\"hello \")\n",
    "# print(emb_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.batch([\"帮我写一个和猫相关的笑话，不超过100字。\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"请帮我写一个关于{topic}的笑话\"\n",
    "\n",
    "# 用法 1\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=template,)\n",
    "\n",
    "# 用法2\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"请帮我写一个关于{topic}的笑话。输出遵循以下格式: {format}\", \n",
    "    partial_variables={\"format\": \"仅仅输出一个笑话，不要输出任何其他内容\"}\n",
    "    )\n",
    "\n",
    "pp = prompt.format(topic=\"猫\")\n",
    "print(pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_test = PromptTemplate.from_template(\"写一个{topic}相关的笑话\")\n",
    "prompt_test.format(topic=\"dog\")\n",
    "\n",
    "\n",
    "prompt_test_verbose = \"\"\"\n",
    "\n",
    "帮我写一个很长很长的笑话帮我写一个很长很长的笑话\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 如果 copilot; 提升 1 倍不夸张\n",
    "# pylance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "examples =[\n",
    "    {\"word\":\"高\", \"antonym\":\"矮\"},\n",
    "    {\"word\":\"胖\",\"antonym\":\"瘦\"},\n",
    "    ]\n",
    "example_template =\"\"\"\n",
    "词语: {word}\n",
    "反义词: {antonym}\\n\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\",\"antonym\"],\n",
    "    template=example_template,)\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples[:1],\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"给出输入词语的反义词\",\n",
    "    suffix=\"词语: {input}\\n反义词:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",)\n",
    "\n",
    "print(few_shot_prompt.format(input=\"美丽\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "few_test = \"\"\"\n",
    "给出输入词语的反义词\n",
    "\n",
    "## example\n",
    "词语: {word} \n",
    "反义词: {antonym}\\n\n",
    "\n",
    "## input\n",
    "词语: {word1} \n",
    "反义词: {antonym2}\\n\n",
    "\n",
    "# 假设有 10 个变量，\n",
    "\"\"\"\n",
    "few_test.format(word1=\"好\", antonym2=\"坏\", word=\"高\", antonym=\"矮\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt, model, output parser\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# output \n",
    "class Output(BaseModel):\n",
    "    query: str\n",
    "    punchline: str\n",
    "\"\"\"\n",
    "我们希望输出\n",
    "{\n",
    "\"query\": \"帮我写一个和猫相关的笑话。\"\n",
    "\"punchline\": \"punchline\".\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template=\"help me write a joke about {topic}。 \\n{format}\", \n",
    "    partial_variables={\"format\": output_parser.get_format_instructions()})\n",
    "chat = ChatOpenAI(base_url=BASE_URL, api_key=API_KEY, model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "chain = prompt | chat | output_parser\n",
    "\n",
    "ans = chain.invoke({\"topic\": \"cat\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=API_KEY, base_url=BASE_URL, model_name=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "tmp = llm(prompt.format(topic=\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.format(topic=\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = tmp + \"你好啊\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# json.loads(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain1 = LLMChain(llm=llm, \n",
    "                 prompt=PromptTemplate.from_template(\n",
    "                     \"帮我的{topic}起一个名字。输出一个名字就可以，不要加其他内容。\"))\n",
    "# chain1.run(\"猫\")\n",
    "second_prompt = PromptTemplate.from_template(\"帮我写一个关于{topic}的诗歌\")\n",
    "chain2 = LLMChain(llm=llm, \n",
    "                 prompt=second_prompt)\n",
    "# chain2.run(\"猫\")\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\"\"\"Simple chain where the outputs of one step feed directly into next.\"\"\"\n",
    "chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "chain.run(\"猫\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"帮我写一个关于{topic}的笑话\")\n",
    "chat = ChatOpenAI(api_key=API_KEY, \n",
    "                  base_url=BASE_URL, \n",
    "                  model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "ans = chain.invoke({\"topic\": \"apple\"})\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github copilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain indexs(retieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "loader = TextLoader(file_path=\"./data/text.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# database = Chroma.from_documents(texts, emb_model)\n",
    "\n",
    "query = \"RAG的本质是什么？\"\n",
    "# retrieval_docs = database.similarity_search(query, k=1)\n",
    "\n",
    "# print(retrieval_docs)\n",
    "# 如何保存在 disk 中\n",
    "# # save to disk\n",
    "db2 = Chroma.from_documents(texts, emb_model, persist_directory=\"./chroma_db\")\n",
    "docs = db2.similarity_search(query)\n",
    "print(docs)\n",
    "# # load from disk\n",
    "# db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=emb_model)\n",
    "# docs = db3.similarity_search(query)\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(texts, emb_model)\n",
    "\n",
    "query = \"RAG的本质是什么？\"\n",
    "retrieval_docs = db.similarity_search(query, k=1)\n",
    "\n",
    "print(retrieval_docs)\n",
    "\n",
    "# faiss save disk\n",
    "# db.save_local(\"faiss_index\")\n",
    "# new_db = FAISS.load_local(\"faiss_index\", emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"mmr\", \n",
    "                            search_kwargs={\"k\": 1, \n",
    "                                           \"score_threshold\": 0.5})\n",
    "retriever.get_relevant_documents(\"RAG的本质是什么\", k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search type 有很多\n",
    "- 相似度\n",
    "- 多样性\n",
    "- MaxMarginalRelevance\n",
    "\n",
    "MMR 根据示例与输入的相似度以及多样性进行选择。它通过找到与输入具有最高余弦相似度的示例，并在此基础上进行迭代添加示例，同时对其与已选择示例的相似度进行惩罚。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True)\n",
    "\n",
    "query = \"RAG的本质是什么?\"\n",
    "result = qa({\n",
    "   \"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(\"RAG的本质是什么?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"所有的北极熊都是白色的\")\n",
    "conversation.predict(input=\"bob是一只北极熊\")\n",
    "conversation.predict(input=\"bob是什么颜色的?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory, ConversationBufferMemory, ConversationSummaryMemory\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=ConversationBufferWindowMemory(k=5))\n",
    "conversation.run(\"What is the capital of Canada?\")\n",
    "# Output: \"The capital of Canada is Ottawa.\"\n",
    "conversation.run(\"What is the capital of France?\")\n",
    "# Output: \"The capital of France is Paris.\"\n",
    "conversation.memory.buffer\n",
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a chatbot having a conversation with a human.\"\n",
    "        ),  # The persistent system prompt\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "        ),  # Where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),  # Where the human input will injected\n",
    "    ]\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chat_llm_chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "chat_llm_chain.predict(human_input=\"Hi there my friend\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"wikipedia\",\"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n",
    "\n",
    "agent.run(\"马斯克（elon musk）的生日是哪天? 到2023年他多少岁了?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 自定义 tool\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "tools = [get_word_length]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but don't know current events\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_with_tools = chat.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"How many letters in the word educa\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_tool_to_openai_function(get_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import get_all_tool_names\n",
    "# get_all_tool_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "tools = [get_word_length]\n",
    "\n",
    "chat.predict_messages([HumanMessage(content=\"How many letters in the word educa\")], \n",
    "                      functions=[format_tool_to_openai_function(get_word_length)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 推荐阅读 openai 怎么调用 function\n",
    "# import openai\n",
    "# import json\n",
    "# from enum import Enum\n",
    "\n",
    "# class BaseTool(Enum):\n",
    "#     Bookkeeping = \"record_price\"\n",
    "#     RecordingTask = \"record_task\"\n",
    "\n",
    "# # record_price是用来给Function Calling调用的函数，\n",
    "# # 这个函数接收两个必填的参数，category类目（string类型），count 数量（int类型）\n",
    "# def record_price(category, count):\n",
    "#     print(category, count)\n",
    "#     print(\"调用获取实时物价的 API\")\n",
    "#     return count*12\n",
    "\n",
    "# def funtion_call_conversation(message):\n",
    "#     messages = [\n",
    "#       {\"role\": \"user\", \"content\": \"今天买了一斤肉，花了多少钱\"}\n",
    "#     ]\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo-0613\",\n",
    "#     messages = message,\n",
    "#     temperature=0,\n",
    "#     functions=[\n",
    "#       {\n",
    "#         \"name\": BaseTool.Bookkeeping.value,\n",
    "#         \"description\": \"返回物品的数量\",\n",
    "#         \"parameters\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"category\": {\"type\": \"string\",\"description\": \"类目\",},\n",
    "#             \"count\": {\"type\": \"integer\", \"description\": \"数量\"},\n",
    "#           },\n",
    "#           \"required\": [\"category\",\"count\"],\n",
    "#         },\n",
    "#       }\n",
    "#     ],\n",
    "#     function_call=\"auto\",\n",
    "#     )\n",
    "#     message = response[\"choices\"][0][\"message\"]\n",
    "#     print(message)\n",
    "#     if(message.get(\"function_call\")):\n",
    "#         function_name = message[\"function_call\"][\"name\"]\n",
    "#         if function_name == BaseTool.Bookkeeping.value:\n",
    "#             arguments = json.loads(message[\"function_call\"][\"arguments\"])\n",
    "#             price = record_price(arguments.get('category'), arguments.get('count'))\n",
    "#             messages.append(message)\n",
    "#             messages.append({\"role\": \"function\", \"name\": BaseTool.Bookkeeping.value, \"content\": price})\n",
    "#             print(messages)\n",
    "#             role_function_conversation(messages)\n",
    "\n",
    "# def role_function_conversation(message):\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo-0613\",\n",
    "#     messages = message,\n",
    "#     temperature=0,\n",
    "#     # name字段表示要调用的函数名，description表示函数描述（让 LLM 读得懂的函数说明）\n",
    "#     # paramters是一个符合JSON Schema格式的对象，用来描述这个函数的入参信息\n",
    "#     functions=[\n",
    "#       {\n",
    "#         \"name\": BaseTool.Bookkeeping.value,\n",
    "#         \"description\": \"返回物品的数量\",\n",
    "#         \"parameters\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"category\": {\"type\": \"string\",\"description\": \"类目\",},\n",
    "#             \"count\": {\"type\": \"integer\", \"description\": \"数量\"},\n",
    "#           },\n",
    "#           \"required\": [\"category\",\"count\"],\n",
    "#         },\n",
    "#       }\n",
    "#     ],\n",
    "#     function_call=\"auto\",\n",
    "#     )\n",
    "#     message = response[\"choices\"][0][\"message\"].get(\"content\")\n",
    "#     print(message)\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     funtion_call_conversation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
