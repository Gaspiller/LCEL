{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "env_config = dotenv_values(\".env\")\n",
    "API_KEY = env_config[\"API_KEY\"]\n",
    "BASE_URL = env_config[\"BASE_URL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n大模型指的是具有大量参数和复杂结构的机器学习模型，可以用来解决复杂的问题，如图像识别、自然语言处理等。这些模型通常需要大量的数据和计算资源来训练，例如深度神经网络模型、支持向量机模型等。大模型可以提高模型的准确性和泛化能力，但也会带来训练和推理的时间、资源成本。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 普通 LLM model \n",
    "from langchain.llms import OpenAI, HuggingFaceHub, HuggingFaceTextGenInference\n",
    "\n",
    "llm = OpenAI(api_key=API_KEY, base_url=BASE_URL, model_name=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "llm(\"大模型是什么？\")\n",
    "\n",
    "\n",
    "# # 如果是其他的模型，这里可以自己去设置一些 hugging\n",
    "# llm = HuggingFaceHub(repo_id =\"NYTK/text-generation-news-gpt2-small-hungarian\",\n",
    "#                       huggingfacehub_api_token=\"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\", \n",
    "#                       model_kwargs={\"temperature\": 0.7}) \n",
    "# llm(\"大模型是什么?\")\n",
    "\n",
    "\n",
    "# # # 使用 本地模型\n",
    "# llm = HuggingFaceTextGenInference(\n",
    "#                 inference_server_url=\"http://localhost:8010/\",\n",
    "#                 max_new_tokens=512,\n",
    "#                 top_k=10,\n",
    "#                 top_p=0.95,\n",
    "#                 typical_p=0.95,\n",
    "#                 temperature=0.01,\n",
    "#                 repetition_penalty=1.03,\n",
    "#             )\n",
    "# print(llm(\"What is Deep Learning?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序员对女友说：“你是我最重要的对象，没有之一。”\n",
      "女友：那你的第二重要的对象是谁？\n",
      "程序员：我的编译器，没有它，我就毫无意义了。\n"
     ]
    }
   ],
   "source": [
    "# 使用 chat 模型\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "chat = ChatOpenAI(api_key=API_KEY, \n",
    "                  base_url=BASE_URL, \n",
    "                  model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "ans = chat.invoke(\"帮我写一个计算机相关的笑话，不要超过 100 个字\")\n",
    "print(ans.content)\n",
    "\n",
    "# # 使用 embedding 模型\n",
    "# emb_model = OpenAIEmbeddings(api_key=env_config.get(\"API_KEY\"), \n",
    "#                              base_url=env_config.get(\"BASE_URL\"))\n",
    "\n",
    "# emb_content = emb_model.embed_query(\"hello \")\n",
    "# print(emb_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='为什么猫咪不喜欢打牌？因为他们总是会被抓住扑克脸！')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.batch([\"帮我写一个和猫相关的笑话，不超过100字。\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请帮我写一个关于猫的笑话。输出遵循以下格式: 仅仅输出一个笑话，不要输出任何其他内容\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"请帮我写一个关于{topic}的笑话\"\n",
    "\n",
    "# 用法 1\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=template,)\n",
    "\n",
    "# 用法2\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"请帮我写一个关于{topic}的笑话。输出遵循以下格式: {format}\", \n",
    "    partial_variables={\"format\": \"仅仅输出一个笑话，不要输出任何其他内容\"}\n",
    "    )\n",
    "\n",
    "pp = prompt.format(topic=\"猫\")\n",
    "print(pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'写一个dog相关的笑话'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_test = PromptTemplate.from_template(\"写一个{topic}相关的笑话\")\n",
    "prompt_test.format(topic=\"dog\")\n",
    "\n",
    "\n",
    "prompt_test_verbose = \"\"\"\n",
    "\n",
    "帮我写一个很长很长的笑话帮我写一个很长很长的笑话\n",
    "\n",
    "帮我写一个很长很长的笑话帮我写一个很长很长的笑话\n",
    "\n",
    "\n",
    "帮我写一个很长很长的笑话\n",
    "帮我写一个很长很长的笑话\n",
    "帮我写一个很长很长的笑话\n",
    "帮我写一个很长很长的笑话\n",
    "帮我写一个很长很长的笑话\n",
    "帮我写一个很长很长的笑话\n",
    "帮我写一个很长很长的笑话\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 如果 copilot; 提升 1 倍不夸张\n",
    "# pylance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给出输入词语的反义词\n",
      "\n",
      "词语: 高\n",
      "反义词: 矮\n",
      "\n",
      "\n",
      "词语: 美丽\n",
      "反义词:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "examples =[\n",
    "    {\"word\":\"高\", \"antonym\":\"矮\"},\n",
    "    {\"word\":\"胖\",\"antonym\":\"瘦\"},\n",
    "    ]\n",
    "example_template =\"\"\"\n",
    "词语: {word}\n",
    "反义词: {antonym}\\n\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\",\"antonym\"],\n",
    "    template=example_template,)\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples[:1],\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"给出输入词语的反义词\",\n",
    "    suffix=\"词语: {input}\\n反义词:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",)\n",
    "\n",
    "print(few_shot_prompt.format(input=\"美丽\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antonym', 'word']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n给出输入词语的反义词\\n\\n## example\\n词语: 高 \\n反义词: 矮\\n\\n\\n## input\\n词语: 好 \\n反义词: 坏\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "few_test = \"\"\"\n",
    "给出输入词语的反义词\n",
    "\n",
    "## example\n",
    "词语: {word} \n",
    "反义词: {antonym}\\n\n",
    "\n",
    "## input\n",
    "词语: {word1} \n",
    "反义词: {antonym2}\\n\n",
    "\n",
    "# 假设有 10 个变量，\n",
    "\"\"\"\n",
    "few_test.format(word1=\"好\", antonym2=\"坏\", word=\"高\", antonym=\"矮\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt, model, output parser\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# output \n",
    "class Output(BaseModel):\n",
    "    query: str\n",
    "    punchline: str\n",
    "\"\"\"\n",
    "我们希望输出\n",
    "{\n",
    "\"query\": \"帮我写一个和猫相关的笑话。\"\n",
    "\"punchline\": \"punchline\".\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template=\"help me write a joke about {topic}。 \\n{format}\", \n",
    "    partial_variables={\"format\": output_parser.get_format_instructions()})\n",
    "chat = ChatOpenAI(base_url=BASE_URL, api_key=API_KEY, model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "chain = prompt | chat | output_parser\n",
    "\n",
    "ans = chain.invoke({\"topic\": \"cat\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output(query=\"Why don't cats play poker in the wild?\", punchline='Too many cheetahs!')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=API_KEY, base_url=BASE_URL, model_name=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "tmp = llm(prompt.format(topic=\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "help me write a joke about cat。 \n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"query\": {\"title\": \"Query\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"query\", \"punchline\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(topic=\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = tmp + \"你好啊\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 3 column 98 (char 99)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain-learn-v1/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain-learn-v1/lib/python3.10/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 3 column 98 (char 99)"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# json.loads(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "小橘\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "小橘儿长在枝头，\n",
      "青翠叶间如一朵，\n",
      "轻轻摇曳风中舞，\n",
      "吸引着小鸟们过来住。\n",
      "\n",
      "它的果实小小的，\n",
      "却散发着甜美的香气，\n",
      "让人忍不住想摘下，\n",
      "品尝一口满口的甜蜜。\n",
      "\n",
      "它的叶子碧绿欲滴，\n",
      "晶莹剔透如翡翠，\n",
      "夏日炎炎来乘凉，\n",
      "小橘儿就是最好的避暑地。\n",
      "\n",
      "秋风吹来果实成熟，\n",
      "红艳艳的挂满枝头，\n",
      "小橘儿笑得更加灿烂，\n",
      "让秋天也变得美妙。\n",
      "\n",
      "冬天来临小橘儿静伏，\n",
      "等待春\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n小橘儿长在枝头，\\n青翠叶间如一朵，\\n轻轻摇曳风中舞，\\n吸引着小鸟们过来住。\\n\\n它的果实小小的，\\n却散发着甜美的香气，\\n让人忍不住想摘下，\\n品尝一口满口的甜蜜。\\n\\n它的叶子碧绿欲滴，\\n晶莹剔透如翡翠，\\n夏日炎炎来乘凉，\\n小橘儿就是最好的避暑地。\\n\\n秋风吹来果实成熟，\\n红艳艳的挂满枝头，\\n小橘儿笑得更加灿烂，\\n让秋天也变得美妙。\\n\\n冬天来临小橘儿静伏，\\n等待春'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain1 = LLMChain(llm=llm, \n",
    "                 prompt=PromptTemplate.from_template(\n",
    "                     \"帮我的{topic}起一个名字。输出一个名字就可以，不要加其他内容。\"))\n",
    "# chain1.run(\"猫\")\n",
    "second_prompt = PromptTemplate.from_template(\"帮我写一个关于{topic}的诗歌\")\n",
    "chain2 = LLMChain(llm=llm, \n",
    "                 prompt=second_prompt)\n",
    "# chain2.run(\"猫\")\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\"\"\"Simple chain where the outputs of one step feed directly into next.\"\"\"\n",
    "chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "chain.run(\"猫\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='为了让你开心，我为你编写了一个关于苹果的笑话：\\n\\n有一天，一个小男孩走进了一家苹果店。他看到一位店员正在忙着整理苹果摊位，于是他好奇地问道：“先生，你卖的苹果是用什么语言说话的呢？”店员笑了笑，回答说：“孩子，苹果是不会说话的。”小男孩皱起了眉头，摇了摇头说：“可是我听说苹果有Siri，那他们应该会说话吧！”店员忍不住笑了出来，他解释道：“孩子，Siri只是苹果的一款智能助手，它并代表着苹果本身。”小男孩恍然大悟，摇摇头说：“原来如此，我还以为苹果会说话呢，那我还是买一个回家吃吧。”'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"帮我写一个关于{topic}的笑话\")\n",
    "chat = ChatOpenAI(api_key=API_KEY, \n",
    "                  base_url=BASE_URL, \n",
    "                  model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "ans = chain.invoke({\"topic\": \"apple\"})\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain 六个模块\n",
    "- Model \n",
    "    - LLM model\n",
    "    - chat model\n",
    "    - emb model\n",
    "- prompt\n",
    "    - input\n",
    "    - output parser prompt\n",
    "- chain\n",
    "    - 自己 LECL 定义\n",
    "        -  prompt | chat | output_part\n",
    "    -  import LLMChain\n",
    "    - xxxxx\n",
    "- Retrieval\n",
    "    - database\n",
    "    - retriver\n",
    "- memory\n",
    "- agent\n",
    "\n",
    "帮我们写了一些胶水代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github copilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain indexs(retieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 186, which is longer than the specified 100\n",
      "Created a chunk of size 165, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='RAG的本质是让模型获取正确的Context(上下文)，利用ICL (In Context Learning)的能力，输出正确的响应。它综合利用了固化在模型权重中的参数化知识和存在外部存储中的非参数化知识(知识库、数据库等)。RAG分为两阶段：使用编码模型（如 BM25、DPR、ColBERT 等）根据问题找到相关的文档。生成阶段：以找到的上下文作为基础，系统生成文本。', metadata={'source': './data/text.txt'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "loader = TextLoader(file_path=\"./data/text.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "database = Chroma.from_documents(texts, emb_model)\n",
    "\n",
    "query = \"RAG的本质是什么？\"\n",
    "retrieval_docs = database.similarity_search(query, k=1)\n",
    "\n",
    "print(retrieval_docs)\n",
    "# 如何保存在 disk 中\n",
    "# # save to disk\n",
    "# db2 = Chroma.from_documents(texts, emb_model, persist_directory=\"./chroma_db\")\n",
    "# docs = db2.similarity_search(query)\n",
    "# # load from disk\n",
    "# db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=emb_model)\n",
    "# docs = db3.similarity_search(query)\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='RAG的本质是让模型获取正确的Context(上下文)，利用ICL (In Context Learning)的能力，输出正确的响应。它综合利用了固化在模型权重中的参数化知识和存在外部存储中的非参数化知识(知识库、数据库等)。RAG分为两阶段：使用编码模型（如 BM25、DPR、ColBERT 等）根据问题找到相关的文档。生成阶段：以找到的上下文作为基础，系统生成文本。', metadata={'source': './data/text.txt'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(texts, emb_model)\n",
    "\n",
    "query = \"RAG的本质是什么？\"\n",
    "retrieval_docs = db.similarity_search(query, k=1)\n",
    "\n",
    "print(retrieval_docs)\n",
    "\n",
    "# faiss save disk\n",
    "# db.save_local(\"faiss_index\")\n",
    "# new_db = FAISS.load_local(\"faiss_index\", emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='RAG的本质是让模型获取正确的Context(上下文)，利用ICL (In Context Learning)的能力，输出正确的响应。它综合利用了固化在模型权重中的参数化知识和存在外部存储中的非参数化知识(知识库、数据库等)。RAG分为两阶段：使用编码模型（如 BM25、DPR、ColBERT 等）根据问题找到相关的文档。生成阶段：以找到的上下文作为基础，系统生成文本。', metadata={'source': './data/text.txt'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_type=\"mmr\", \n",
    "                            search_kwargs={\"k\": 1, \n",
    "                                           \"score_threshold\": 0.5})\n",
    "retriever.get_relevant_documents(\"RAG的本质是什么\", k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search type 有很多\n",
    "- 相似度\n",
    "- 多样性\n",
    "- MaxMarginalRelevance\n",
    "\n",
    "MMR 根据示例与输入的相似度以及多样性进行选择。它通过找到与输入具有最高余弦相似度的示例，并在此基础上进行迭代添加示例，同时对其与已选择示例的相似度进行惩罚。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAG的本质是让模型获取正确的Context(上下文)，利用ICL (In Context Learning)的能力，输出正确的响应。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True)\n",
    "\n",
    "query = \"RAG的本质是什么?\"\n",
    "result = qa({\n",
    "   \"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RAG（Red, Amber, Green）是一种用于标记和评估项目、任务或进展状态的简单和直观的方法。它通过将项目分为三个颜色级别来提供一个快速的视觉指示，以帮助团队成员和利益相关者理解项目的状态和进展情况。\n",
      "\n",
      "其本质是提供了一种有效的沟通工具，帮助团队和利益相关者共同关注和解决项目中最重要的问题。它可以帮助团队更好地管理风险、识别问题并及时采取行动，从而提高项目的成功率。\n",
      "\n",
      "另外，RAG还可以促进团队的协作和交流，帮助团队成员更好地了解彼此的工作情况，从而提高团队的效率和效能。\n",
      "\n",
      "总的来说，RAG的本质是提供\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"RAG的本质是什么?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 所有的北极熊都是白色的\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 所有的北极熊都是白色的\n",
      "AI:  是的，大部分的北极熊都是白色的，這是因為牠們的毛髮可以反射陽光，讓牠們在白雪覆蓋的環境中更容易隱藏自己。不過，其實也有一小部分的北极熊是黑色的，這種顏色的北极熊被稱為“黑熊型”。這種黑色的北极熊主要生活在西伯利亞和阿拉斯加地區，牠們的毛髮顏色較暗是因為遺傳因素，並且在白色毛髮的北极熊中也會偶爾出現。不過，無論是白色還是黑色的北极熊，都\n",
      "Human: bob是一只北极熊\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 所有的北极熊都是白色的\n",
      "AI:  是的，大部分的北极熊都是白色的，這是因為牠們的毛髮可以反射陽光，讓牠們在白雪覆蓋的環境中更容易隱藏自己。不過，其實也有一小部分的北极熊是黑色的，這種顏色的北极熊被稱為“黑熊型”。這種黑色的北极熊主要生活在西伯利亞和阿拉斯加地區，牠們的毛髮顏色較暗是因為遺傳因素，並且在白色毛髮的北极熊中也會偶爾出現。不過，無論是白色還是黑色的北极熊，都\n",
      "Human: bob是一只北极熊\n",
      "AI:  我無法確定bob是否是一只北极熊，因為我沒有足夠的上下文信息來判斷。可能這只bob是一隻北极熊，也可能是人類或其他動物的名字。如果您能提供更多信息，我可以更準確地回答您的問題。\n",
      "Human: bob是什么颜色的?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  很抱歉，我無法回答這個問題，因為我沒有足夠的上下文信息來回答。如果bob是一隻北极熊，那麼我們可以推測牠可能是白色或黑色的。但如果bob是其他動物或人類，則無法確定牠的顏色是什麼。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"所有的北极熊都是白色的\")\n",
    "conversation.predict(input=\"bob是一只北极熊\")\n",
    "conversation.predict(input=\"bob是什么颜色的?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[HumanMessage(content='hi!'), AIMessage(content='whats up?')])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What is the capital of Canada?\\nAI:  The capital of Canada is Ottawa. It is located in the province of Ontario and is situated on the Ottawa River. Ottawa was originally named Bytown and was founded in 1826 as a logging town. It was officially chosen as the capital of Canada in 1857 by Queen Victoria. Ottawa is home to many important government buildings and institutions, including the Parliament Buildings and the Supreme Court of Canada. It has a population of approximately 1 million people and is known for its beautiful parks, museums, and cultural events. Is there anything else you would like to know about Ottawa or Canada in general?\\nHuman: What is the capital of France?\\nAI:  The capital of France is Paris. It is located in the Île-de-France region and is situated on the River Seine. Paris is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. It has a population of approximately 2.2 million people and is considered a major cultural, economic, and political center in Europe. Is there anything else you would like to know about Paris or France in general?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory, ConversationBufferMemory, ConversationSummaryMemory\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=ConversationBufferWindowMemory(k=5))\n",
    "conversation.run(\"What is the capital of Canada?\")\n",
    "# Output: \"The capital of Canada is Ottawa.\"\n",
    "conversation.run(\"What is the capital of France?\")\n",
    "# Output: \"The capital of France is Paris.\"\n",
    "conversation.memory.buffer\n",
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is the capital of Canada?\n",
      "AI:  The capital of Canada is Ottawa. It is located in the province of Ontario and is situated on the Ottawa River. Ottawa was originally named Bytown and was founded in 1826 as a logging town. It was officially chosen as the capital of Canada in 1857 by Queen Victoria. Ottawa is home to many important government buildings and institutions, including the Parliament Buildings and the Supreme Court of Canada. It has a population of approximately 1 million people and is known for its beautiful parks, museums, and cultural events. Is there anything else you would like to know about Ottawa or Canada in general?\n",
      "Human: What is the capital of France?\n",
      "AI:  The capital of France is Paris. It is located in the Île-de-France region and is situated on the River Seine. Paris is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. It has a population of approximately 2.2 million people and is considered a major cultural, economic, and political center in Europe. Is there anything else you would like to know about Paris or France in general?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human.\n",
      "Human: Hi there my friend\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a chatbot having a conversation with a human.\"\n",
    "        ),  # The persistent system prompt\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "        ),  # Where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),  # Where the human input will injected\n",
    "    ]\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chat_llm_chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "chat_llm_chain.predict(human_input=\"Hi there my friend\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m We can use Wikipedia or Calculator to find this information\n",
      "Action: Wikipedia\n",
      "Action Input: 马斯克\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Taxkorgan Tajik Autonomous County\n",
      "Summary: Taxkorgan Tajik Autonomous County (often shortened to Taxkorgan County and also spelled Tashkurgan) is an autonomous county of Kashgar Prefecture, in western Xinjiang, China. The county seat is Tashkurgan. The county is the only Tajik (Pamiri) autonomous county in China.\n",
      "\n",
      "Page: Ili Kazakh Autonomous Prefecture\n",
      "Summary: Ili Kazakh Autonomous Prefecture is an autonomous prefecture in northern Xinjiang, China. Its capital is Yining, also known as Ghulja or Kulja.\n",
      "Covering an area of 268,591 square kilometres (16.18 per cent of Xinjiang), Ili Prefecture shares a 2,019 kilometre-long border with Kazakhstan, Mongolia, and Russia. There are nine ports of entry in Ili Prefecture at the national level, including Jeminay and Khorgas. Directly administered regions (Chinese: 直辖区域) within the prefecture cover 56,622 square kilometres (21.08 per cent of Ili's total area) and have a population of 4,930,600 (63.95 per cent of Ili's registered population). Kazakhs in China are the second largest ethnicity in the prefecture after the Han, and make up a little over a quarter of the population.\n",
      "Ili is the only prefecture-level division that has other prefecture-level divisions (Altay and Tacheng Prefectures) under its administration. The term \"sub-provincial autonomous prefecture\" (副省级自治州) has often been applied to Ili, but it has no legal basis under Chinese law and is a misnomer.\n",
      "\n",
      "Page: Sherlock Holmes and the Great Escape\n",
      "Summary: Sherlock Holmes and the Great Escape, originally titled The Great Detective Sherlock Holmes – The Greatest Jail Breaker, is a 2019 Hong Kong animated film based on the children's book series The Great Detective Sherlock Holmes. Toe Yuen and Matthew Chow co-directed this film. Some investments into the film came from the company Golden Scene. The film was released theatrically in 2019.Shout! Factory released an English dub in Canada and the United States on DVD on 23 March 2021.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This information is not related to Elon Musk's birthday. Let's try using Calculator instead.\n",
      "Action: Calculator\n",
      "Action Input: 2023-1971\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 52\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m We now know that Elon Musk will be 52 years old in 2023.\n",
      "Final Answer: Elon Musk's birthday is June 28, 1971. In 2023, he will be 52 years old.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Elon Musk's birthday is June 28, 1971. In 2023, he will be 52 years old.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"wikipedia\",\"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n",
    "\n",
    "agent.run(\"马斯克（elon musk）的生日是哪天? 到2023年他多少岁了?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_word_length` with `{'word': 'educa'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m5\u001b[0m\u001b[32;1m\u001b[1;3mThere are 5 letters in the word \"educa\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How many letters in the word educa',\n",
       " 'output': 'There are 5 letters in the word \"educa\".'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 自定义 tool\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "tools = [get_word_length]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but don't know current events\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_with_tools = chat.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"How many letters in the word educa\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_word_length',\n",
       " 'description': 'get_word_length(word: str) -> int - Returns the length of a word.',\n",
       " 'parameters': {'title': 'get_word_lengthSchemaSchema',\n",
       "  'type': 'object',\n",
       "  'properties': {'word': {'title': 'Word', 'type': 'string'}},\n",
       "  'required': ['word']}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_tool_to_openai_function(get_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import get_all_tool_names\n",
    "# get_all_tool_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"word\": \"educa\"\\n}', 'name': 'get_word_length'}})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "tools = [get_word_length]\n",
    "\n",
    "chat.predict_messages([HumanMessage(content=\"How many letters in the word educa\")], \n",
    "                      functions=[format_tool_to_openai_function(get_word_length)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 推荐阅读 openai 怎么调用 function\n",
    "# import openai\n",
    "# import json\n",
    "# from enum import Enum\n",
    "\n",
    "# class BaseTool(Enum):\n",
    "#     Bookkeeping = \"record_price\"\n",
    "#     RecordingTask = \"record_task\"\n",
    "\n",
    "# # record_price是用来给Function Calling调用的函数，\n",
    "# # 这个函数接收两个必填的参数，category类目（string类型），count 数量（int类型）\n",
    "# def record_price(category, count):\n",
    "#     print(category, count)\n",
    "#     print(\"调用获取实时物价的 API\")\n",
    "#     return count*12\n",
    "\n",
    "# def funtion_call_conversation(message):\n",
    "#     messages = [\n",
    "#       {\"role\": \"user\", \"content\": \"今天买了一斤肉，花了多少钱\"}\n",
    "#     ]\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo-0613\",\n",
    "#     messages = message,\n",
    "#     temperature=0,\n",
    "#     functions=[\n",
    "#       {\n",
    "#         \"name\": BaseTool.Bookkeeping.value,\n",
    "#         \"description\": \"返回物品的数量\",\n",
    "#         \"parameters\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"category\": {\"type\": \"string\",\"description\": \"类目\",},\n",
    "#             \"count\": {\"type\": \"integer\", \"description\": \"数量\"},\n",
    "#           },\n",
    "#           \"required\": [\"category\",\"count\"],\n",
    "#         },\n",
    "#       }\n",
    "#     ],\n",
    "#     function_call=\"auto\",\n",
    "#     )\n",
    "#     message = response[\"choices\"][0][\"message\"]\n",
    "#     print(message)\n",
    "#     if(message.get(\"function_call\")):\n",
    "#         function_name = message[\"function_call\"][\"name\"]\n",
    "#         if function_name == BaseTool.Bookkeeping.value:\n",
    "#             arguments = json.loads(message[\"function_call\"][\"arguments\"])\n",
    "#             price = record_price(arguments.get('category'), arguments.get('count'))\n",
    "#             messages.append(message)\n",
    "#             messages.append({\"role\": \"function\", \"name\": BaseTool.Bookkeeping.value, \"content\": price})\n",
    "#             print(messages)\n",
    "#             role_function_conversation(messages)\n",
    "\n",
    "# def role_function_conversation(message):\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo-0613\",\n",
    "#     messages = message,\n",
    "#     temperature=0,\n",
    "#     # name字段表示要调用的函数名，description表示函数描述（让 LLM 读得懂的函数说明）\n",
    "#     # paramters是一个符合JSON Schema格式的对象，用来描述这个函数的入参信息\n",
    "#     functions=[\n",
    "#       {\n",
    "#         \"name\": BaseTool.Bookkeeping.value,\n",
    "#         \"description\": \"返回物品的数量\",\n",
    "#         \"parameters\": {\n",
    "#           \"type\": \"object\",\n",
    "#           \"properties\": {\n",
    "#             \"category\": {\"type\": \"string\",\"description\": \"类目\",},\n",
    "#             \"count\": {\"type\": \"integer\", \"description\": \"数量\"},\n",
    "#           },\n",
    "#           \"required\": [\"category\",\"count\"],\n",
    "#         },\n",
    "#       }\n",
    "#     ],\n",
    "#     function_call=\"auto\",\n",
    "#     )\n",
    "#     message = response[\"choices\"][0][\"message\"].get(\"content\")\n",
    "#     print(message)\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     funtion_call_conversation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-learn-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
