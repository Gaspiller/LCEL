{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf5c67b-28d0-4607-8568-7c90ff408869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "125cd22b-cc45-408f-9818-92451d64ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qkv_attention_value(q, k, v, mask = False):\n",
    "\n",
    "    k = torch.transpose(k, dim0 = -2, dim1 = -1)\n",
    "    scores = torch.matmul(q, k)\n",
    "\n",
    "    if isinstance(mask, bool):\n",
    "        if mask:\n",
    "            _shape = scores.shape\n",
    "            mask = torch.ones((_shape[-2], _shape[-1]))\n",
    "            mask = mask[None][None]\n",
    "        else:\n",
    "            mask = None\n",
    "    if mask is not None:\n",
    "        scores = scores + mask\n",
    "\n",
    "    alpha = torch.softmax(scoresm, dim = -1)\n",
    "\n",
    "    v = torch.matmul(alpha, v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d67d0d5b-b4a8-4a04-8b9b-26c3b529db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_header):\n",
    "        super(MultiHeadSeflAttention, self).__init__()\n",
    "        assert hidden_size % num_header == 0, f\"header的数目无法整除:{hidden_size}, {num_header}\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_header = num_header\n",
    "        self.wq = nn.Sequential(\n",
    "            nn.Linear(in_features = self.hidden_size, out_features = self.hidden_size)\n",
    "        )\n",
    "        self.wk = nn.Sequential(\n",
    "            nn.Linear(in_features = self.hidden_size, out_features = self.hidden_size)\n",
    "        )\n",
    "        self.wv = nn.Sequential(\n",
    "            nn.Linear(in_features = self.hidden_size, out_features = self.hidden_size)\n",
    "        )\n",
    "        self.wo = nn.Sequential(\n",
    "            nn.Linear(in_features = self.hidden_size, out_features = self.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def split(self, vs):\n",
    "        n, t, e = vs.shape\n",
    "        vs = torch.reshape(vs, shape = (n, t, self.num_header, e // self.num_header))\n",
    "        vs = torch.permute(vs, dims = (0,2,1,3))\n",
    "        return vs\n",
    "    def forward(self, x, attention_mask = None, **kwargs):\n",
    "        q = self.wq(x)  # [n,t,e]\n",
    "        k = self.wk(x)  # [n,t,e]\n",
    "        v = self.wv(x)  # [n,t,e]\n",
    "        q = self.split(q)  # [n,t,e] --> [n,h,t,v]  e=h*v h就是head的数目，v就是每个头中self-attention的维度大小\n",
    "        k = self.split(k)  # [n,t,e] --> [n,h,t,v]  e=h*v\n",
    "        v = self.split(v)  # [n,t,e] --> [n,h,t,v]  e=h*v\n",
    "\n",
    "        v = qkv_attention_value(q, k, v, attention_mask)\n",
    "\n",
    "        v = torch.permute(v, dims=(0, 2, 1, 3))  # [n,h,t,v] --> [n,t,h,v]\n",
    "        n, t, _, _ = v.shape\n",
    "        v = torch.reshape(v, shape=(n, t, -1))  # [n,t,h,v] -> [n,t,e]\n",
    "        v = self.wo(v)  # 多个头之间的特征组合合并\n",
    "        return v\n",
    "\n",
    "class MultiHeadEncoderDecoderAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_header):\n",
    "        super(MultiHeadEncoderDecoderAttention, self).__init__()\n",
    "        assert hidden_size % num_header == 0, f\"header的数目没办法整除:{hidden_size}, {num_header}\"\n",
    "\n",
    "        self.hidden_size = hidden_size  # 就是向量维度大小，也就是E\n",
    "        self.num_header = num_header  # 头的数目\n",
    "\n",
    "        self.wo = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def split(self, vs):\n",
    "        n, t, e = vs.shape\n",
    "        vs = torch.reshape(vs, shape=(n, t, self.num_header, e // self.num_header))\n",
    "        vs = torch.permute(vs, dims=(0, 2, 1, 3))\n",
    "        return vs\n",
    "\n",
    "    def forward(self, q, encoder_k, encoder_v, encoder_attention_mask, **kwargs):\n",
    "        \"\"\"\n",
    "        编码器解码器attention\n",
    "        :param q: [N,T1,E]\n",
    "        :param encoder_k: [N,T2,E]\n",
    "        :param encoder_v: [N,T2,E]\n",
    "        :param encoder_attention_mask: [N,1,T2,T2]\n",
    "        :return: [N,T1,E]\n",
    "        \"\"\"\n",
    "        q = self.split(q)  # [n,t,e] --> [n,h,t,v]  e=h*v h就是head的数目，v就是每个头中self-attention的维度大小\n",
    "        k = self.split(encoder_k)  # [n,t,e] --> [n,h,t,v]  e=h*v\n",
    "        v = self.split(encoder_v)  # [n,t,e] --> [n,h,t,v]  e=h*v\n",
    "\n",
    "        # 计算attention value\n",
    "        v = qkv_attention_value(q, k, v, mask=encoder_attention_mask)\n",
    "\n",
    "        # 5. 输出\n",
    "        v = torch.permute(v, dims=(0, 2, 1, 3))  # [n,h,t,v] --> [n,t,h,v]\n",
    "        n, t, _, _ = v.shape\n",
    "        v = torch.reshape(v, shape=(n, t, -1))  # [n,t,h,v] -> [n,t,e]\n",
    "        v = self.wo(v)  # 多个头之间的特征组合合并\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4095d00-9003-4036-9731-55be8d6400ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(FFN, self).__init__()\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ee7a642-9308-4eee-b04f-4b0c8f0afe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualsNorm(nn.Module):\n",
    "    def __init__(self, block, hidden_size):\n",
    "        super(ResidualsNorm, self).__init__()\n",
    "        self.block = block\n",
    "        self.norm = nn.LayerNorm(normalized_shape=hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        z = self.block(x, **kwargs)\n",
    "        z = self.relu(x + z)\n",
    "        z = self.norm(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29d4d181-b482-4fd4-abfd-3b1fbd31362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayers(nn.Module):\n",
    "    def __init__(self, hidden_size, num_header, encoder_layers):\n",
    "        super(TransformerEncoderLayers, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(encoder_layers):\n",
    "            layer = [\n",
    "                ResidualsNorm(\n",
    "                    block=MultiHeadSelfAttention(hidden_size=hidden_size, num_header=num_header),\n",
    "                    hidden_size=hidden_size\n",
    "                ),\n",
    "                ResidualsNorm(\n",
    "                    block=FFN(hidden_size=hidden_size),\n",
    "                    hidden_size=hidden_size\n",
    "                )\n",
    "            ]\n",
    "            layers.extend(layer)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        attention_mask = torch.unsqueeze(attention_mask, dim=1)  # 增加header维度\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_header, max_seq_length, encoder_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.input_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)\n",
    "        self.position_emb = nn.Embedding(num_embeddings=max_seq_length, embedding_dim=hidden_size)\n",
    "        self.layers = TransformerEncoderLayers(hidden_size, num_header, encoder_layers)\n",
    "\n",
    "    def forward(self, input_token_ids, input_position_ids, input_mask):\n",
    "        \"\"\"\n",
    "        前向过程\n",
    "        :param input_token_ids: [N,T] long类型的token id\n",
    "        :param input_position_ids: [N,T] long类型的位置id\n",
    "        :param input_mask: [N,T,T] float类型的mask矩阵\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 1. 获取token的embedding\n",
    "        inp_embedding = self.input_emb(input_token_ids)  # [N,T,E]\n",
    "\n",
    "        # 2. 获取位置embedding\n",
    "        position_embedding = self.position_emb(input_position_ids)\n",
    "\n",
    "        # 3. 合并embedding\n",
    "        emd = inp_embedding + position_embedding\n",
    "\n",
    "        # 4. 输入到attention提取特征\n",
    "        feat_emd = self.layers(emd, attention_mask=input_mask)\n",
    "\n",
    "        return feat_emd\n",
    "\n",
    "\n",
    "class TransformerDecoderLayers(nn.Module):\n",
    "    def __init__(self, hidden_size, num_header, decoder_layers):\n",
    "        super(TransformerDecoderLayers, self).__init__()\n",
    "\n",
    "        self.wk = nn.Linear(hidden_size, hidden_size)\n",
    "        self.wv = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(decoder_layers):\n",
    "            layer = [\n",
    "                ResidualsNorm(\n",
    "                    block=MultiHeadSelfAttention(hidden_size=hidden_size, num_header=num_header),\n",
    "                    hidden_size=hidden_size\n",
    "                ),\n",
    "                ResidualsNorm(\n",
    "                    block=MultiHeadEncoderDecoderAttention(hidden_size=hidden_size, num_header=num_header),\n",
    "                    hidden_size=hidden_size\n",
    "                ),\n",
    "                ResidualsNorm(\n",
    "                    block=FFN(hidden_size=hidden_size),\n",
    "                    hidden_size=hidden_size\n",
    "                )\n",
    "            ]\n",
    "            layers.extend(layer)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, encoder_outputs=None, encoder_attention_mask=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        :param x: [N,T2,E]\n",
    "        :param encoder_outputs: [N,T1,E]\n",
    "        :param encoder_attention_mask: [N,1,T1]\n",
    "        :param attention_mask: [N,T2,T2]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        attention_mask = torch.unsqueeze(attention_mask, dim=1)  # 增加header维度 [N,T2,T2] -> [N,1,T2,T2]\n",
    "        encoder_attention_mask = torch.unsqueeze(encoder_attention_mask, dim=1)  # 增加header维度 [N,1,T1] -> [N,1,1,T1]\n",
    "        k = self.wk(encoder_outputs)  # [N,T1,E]\n",
    "        v = self.wv(encoder_outputs)  # [N,T1,E]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                encoder_k=k, encoder_v=v, encoder_attention_mask=encoder_attention_mask,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_header, max_seq_length, decoder_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.input_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size)\n",
    "        self.position_emb = nn.Embedding(num_embeddings=max_seq_length, embedding_dim=hidden_size)\n",
    "        self.layers = TransformerDecoderLayers(hidden_size, num_header, decoder_layers)\n",
    "\n",
    "    def forward(self, input_token_ids, input_position_ids, input_mask, encoder_outputs, encoder_attention_mask):\n",
    "        \"\"\"\n",
    "        前向过程\n",
    "        :param input_token_ids: [N,T] long类型的token id\n",
    "        :param input_position_ids: [N,T] long类型的位置id\n",
    "        :param input_mask: [N,T,T] float类型的mask矩阵\n",
    "        :param encoder_outputs: [N,T1,E] 编码器的输出状态信息\n",
    "        :param encoder_attention_mask: [N,T1,T1] 编码器的输入mask信息\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # 1. 获取token的embedding\n",
    "            inp_embedding = self.input_emb(input_token_ids)  # [N,T,E]\n",
    "\n",
    "            # 2. 获取位置embedding\n",
    "            position_embedding = self.position_emb(input_position_ids)\n",
    "\n",
    "            # 3. 合并embedding\n",
    "            emd = inp_embedding + position_embedding\n",
    "\n",
    "            # 4. 输入到attention提取特征\n",
    "            feat_emd = self.layers(\n",
    "                emd, encoder_outputs=encoder_outputs,\n",
    "                encoder_attention_mask=encoder_attention_mask, attention_mask=input_mask\n",
    "            )\n",
    "\n",
    "            return feat_emd\n",
    "        else:\n",
    "            raise ValueError(\"当前模拟代码不实现推理过程，仅实现training过程\")\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def forward(self, encoder_input_ids, encoder_lengths, label_ids=None, label_lengths=None):\n",
    "        \"\"\"\n",
    "        :param encoder_input_ids: 编码器输入token id: [N,T1]\n",
    "        :param encoder_lengths: 编码器输入的文本实际长度值, [N,] 也可以直接外部传入mask等信息，但是至少要有代码实现从length到mask的转换\n",
    "        :param label_ids: 模型预测期望输出的token id: [N,T2]\n",
    "        :param label_lengths: 模型预测期望输出文本实际长度, [N,]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "368592f3-0c4b-4384-987f-dc1191ea6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer():\n",
    "    encoder = TransformerEncoder(vocab_size=1000, hidden_size=512, num_header=8, max_seq_length=1024, encoder_layers=6)\n",
    "    decoder = TransformerDecoder(vocab_size=1000, hidden_size=512, num_header=8, max_seq_length=1024, decoder_layers=6)\n",
    "\n",
    "    input_token_ids = torch.tensor([\n",
    "        [100, 102, 108, 253, 125],  # 第一个样本实际长度为5\n",
    "        [254, 125, 106, 0, 0]  # 第二个样本实际长度为3\n",
    "    ])\n",
    "    input_position_ids = torch.tensor([\n",
    "        [0, 1, 2, 3, 4],\n",
    "        [0, 1, 2, 3, 4]\n",
    "    ])\n",
    "    input_mask = torch.tensor([\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        ],\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "            [-10000.0, -10000.0, -10000.0, 0.0, -10000.0],\n",
    "            [-10000.0, -10000.0, -10000.0, -10000.0, 0.0],\n",
    "        ],\n",
    "    ])\n",
    "    encoder_attention_mask = torch.tensor([\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0]  # 表示第一个样本的解码器中第一个时刻和编码器的各个时刻之间的mask值\n",
    "        ],\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0]  # 是因为编码器的输入中，最后两个位置是填充\n",
    "        ],\n",
    "    ])\n",
    "\n",
    "    input_decoder_token_ids = torch.tensor([\n",
    "        [251, 235, 124, 321, 25, 68],\n",
    "        [351, 235, 126, 253, 0, 0]\n",
    "    ])\n",
    "    input_decoder_position_ids = torch.tensor([\n",
    "        [0, 1, 2, 3, 4, 5],\n",
    "        [0, 1, 2, 3, 4, 5]\n",
    "    ])\n",
    "    input_decoder_mask = torch.tensor([\n",
    "        [\n",
    "            [0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        ],\n",
    "        [\n",
    "            [0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0],\n",
    "            [0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0],\n",
    "            [-10000.0, -10000.0, -10000.0, -10000.0, 0.0, -10000.0],\n",
    "            [-10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0]\n",
    "        ],\n",
    "    ])\n",
    "\n",
    "    encoder_outputs = encoder(input_token_ids, input_position_ids, input_mask)\n",
    "    print(encoder_outputs.shape)\n",
    "\n",
    "    decoder_outputs = decoder(\n",
    "        input_token_ids=input_decoder_token_ids,\n",
    "        input_position_ids=input_decoder_position_ids,\n",
    "        input_mask=input_decoder_mask,\n",
    "        encoder_outputs=encoder_outputs,\n",
    "        encoder_attention_mask=encoder_attention_mask\n",
    "    )\n",
    "    print(decoder_outputs.shape)\n",
    "\n",
    "def model_structure(model):\n",
    "    blank = ' '\n",
    "    print('-' * 90)\n",
    "    print('|' + ' ' * 11 + 'weight name' + ' ' * 10 + '|' \\\n",
    "          + ' ' * 15 + 'weight shape' + ' ' * 15 + '|' \\\n",
    "          + ' ' * 3 + 'number' + ' ' * 3 + '|')\n",
    "    print('-' * 90)\n",
    "    num_para = 0\n",
    "    type_size = 1  # 如果是浮点数就是4\n",
    " \n",
    "    for index, (key, w_variable) in enumerate(model.named_parameters()):\n",
    "        if len(key) <= 30:\n",
    "            key = key + (30 - len(key)) * blank\n",
    "        shape = str(w_variable.shape)\n",
    "        if len(shape) <= 40:\n",
    "            shape = shape + (40 - len(shape)) * blank\n",
    "        each_para = 1\n",
    "        for k in w_variable.shape:\n",
    "            each_para *= k\n",
    "        num_para += each_para\n",
    "        str_num = str(each_para)\n",
    "        if len(str_num) <= 10:\n",
    "            str_num = str_num + (10 - len(str_num)) * blank\n",
    " \n",
    "        print('| {} | {} | {} |'.format(key, shape, str_num))\n",
    "    print('-' * 90)\n",
    "    print('The total number of parameters: ' + str(num_para))\n",
    "    print('The parameters of Model {}: {:4f}M'.format(model._get_name(), num_para * type_size / 1000 / 1000))\n",
    "    print('-' * 90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb5a4cbf-d435-4d23-b78c-e20df0e2adf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     transformer()\n\u001b[0;32m      3\u001b[0m     net\u001b[38;5;241m=\u001b[39mTransformerDecoder(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(net)\n",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m, in \u001b[0;36mtransformer\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransformer\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m TransformerEncoder(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, num_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, encoder_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      3\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m TransformerDecoder(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, num_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, decoder_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      5\u001b[0m     input_token_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[0;32m      6\u001b[0m         [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m102\u001b[39m, \u001b[38;5;241m108\u001b[39m, \u001b[38;5;241m253\u001b[39m, \u001b[38;5;241m125\u001b[39m],  \u001b[38;5;66;03m# 第一个样本实际长度为5\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         [\u001b[38;5;241m254\u001b[39m, \u001b[38;5;241m125\u001b[39m, \u001b[38;5;241m106\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# 第二个样本实际长度为3\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     ])\n",
      "Cell \u001b[1;32mIn[26], line 33\u001b[0m, in \u001b[0;36mTransformerEncoder.__init__\u001b[1;34m(self, vocab_size, hidden_size, num_header, max_seq_length, encoder_layers)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(num_embeddings\u001b[38;5;241m=\u001b[39mvocab_size, embedding_dim\u001b[38;5;241m=\u001b[39mhidden_size)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(num_embeddings\u001b[38;5;241m=\u001b[39mmax_seq_length, embedding_dim\u001b[38;5;241m=\u001b[39mhidden_size)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m TransformerEncoderLayers(hidden_size, num_header, encoder_layers)\n",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m, in \u001b[0;36mTransformerEncoderLayers.__init__\u001b[1;34m(self, hidden_size, num_header, encoder_layers)\u001b[0m\n\u001b[0;32m      5\u001b[0m layers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(encoder_layers):\n\u001b[0;32m      7\u001b[0m     layer \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m         ResidualsNorm(\n\u001b[1;32m----> 9\u001b[0m             block\u001b[38;5;241m=\u001b[39mMultiHeadSelfAttention(hidden_size\u001b[38;5;241m=\u001b[39mhidden_size, num_header\u001b[38;5;241m=\u001b[39mnum_header),\n\u001b[0;32m     10\u001b[0m             hidden_size\u001b[38;5;241m=\u001b[39mhidden_size\n\u001b[0;32m     11\u001b[0m         ),\n\u001b[0;32m     12\u001b[0m         ResidualsNorm(\n\u001b[0;32m     13\u001b[0m             block\u001b[38;5;241m=\u001b[39mFFN(hidden_size\u001b[38;5;241m=\u001b[39mhidden_size),\n\u001b[0;32m     14\u001b[0m             hidden_size\u001b[38;5;241m=\u001b[39mhidden_size\n\u001b[0;32m     15\u001b[0m         )\n\u001b[0;32m     16\u001b[0m     ]\n\u001b[0;32m     17\u001b[0m     layers\u001b[38;5;241m.\u001b[39mextend(layer)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(layers)\n",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.__init__\u001b[1;34m(self, hidden_size, num_header)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_size, num_header):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(MultiHeadSeflAttention, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m hidden_size \u001b[38;5;241m%\u001b[39m num_header \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader的数目无法整除:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_header\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m hidden_size\n",
      "\u001b[1;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    transformer()\n",
    "    net=TransformerDecoder(1,8,8,20,5)\n",
    "    print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29212d-1820-465f-abe4-70cf68409616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
